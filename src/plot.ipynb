{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc,precision_recall_curve,f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = \"NPM1_K\"\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "csv_path = f\"/home/weber50432/AML_image_processing/MIL_slide_level/output/{job}/predictions.csv\"\n",
    "df = pd.read_csv(csv_path, sep=\",\", encoding=\"utf-8\")\n",
    "# Get the true labels and predicted labels\n",
    "y_true = df['target']\n",
    "y_pred = df['prediction']\n",
    "# Get the gene mutation probabilities\n",
    "y_score = df['probability']\n",
    "cm = metrics.confusion_matrix(y_true, y_pred, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])\n",
    "# Compute the false positive rate, true positive rate and thresholds for the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "# Compute the area under the ROC curve\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# show the accuracy, precision, recall and f1-score\n",
    "# print(metrics.classification_report(y_true, y_pred))\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "# set the title of the figure\n",
    "fig.suptitle(f'{job} prediction results', fontsize=16)\n",
    "# Plot the confusion matrix in the first subplot\n",
    "cm_display.plot(ax=axs[0][0], cmap=plt.cm.Blues)\n",
    "axs[0][0].set_title('confusion matrix')\n",
    "axs[0][0].text(-0.15, 1.1, '(a)', transform=axs[0][0].transAxes, size=14)\n",
    "# Plot the ROC curve in the second subplot\n",
    "axs[0][1].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "axs[0][1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axs[0][1].set_xlim([0.0, 1.0])\n",
    "axs[0][1].set_ylim([0.0, 1.05])\n",
    "axs[0][1].set_xlabel('False Positive Rate')\n",
    "axs[0][1].set_ylabel('True Positive Rate')\n",
    "axs[0][1].set_title('ROC')\n",
    "axs[0][1].legend(loc=\"lower right\")\n",
    "axs[0][1].text(-0.15, 1, '(b)', transform=axs[0][1].transAxes, size=14)\n",
    "\n",
    "# Plot the precision-recall curve in the third subplot\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "axs[1][0].plot(recall, precision, color='darkorange', lw=2, label='PR curve (F1 = %0.2f)' % f1)\n",
    "axs[1][0].set_xlim([0.0, 1.05])\n",
    "axs[1][0].set_ylim([0.0, 1.05])\n",
    "axs[1][0].set_xlabel('Recall')\n",
    "axs[1][0].set_ylabel('Precision')\n",
    "axs[1][0].text(-0.12, 1.1, '(c)', transform=axs[1][0].transAxes, size=14)\n",
    "axs[1][0].legend(loc=\"lower right\")\n",
    "axs[1][0].set_title('Precision-Recall curve')\n",
    "# show the accuracy, precision, recall and f1-score in a table form at the fourth subplot\n",
    "report = metrics.classification_report(y_true, y_pred, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report.reset_index(drop=False, inplace=True)\n",
    "df_report.rename(columns={'index':''}, inplace=True)\n",
    "df_report.replace(to_replace=\"0\", value='False', inplace=True)\n",
    "df_report.replace(to_replace=\"1\", value='True', inplace=True)\n",
    "# remove the last two rows(avg)\n",
    "df_report.drop(df_report.tail(2).index, inplace=True)\n",
    "df_report = df_report.round(2)\n",
    "axs[1][1].text(-0.12,0.9, '(d)', transform=axs[1][1].transAxes, size=14)\n",
    "axs[1][1].axis('tight')\n",
    "axs[1][1].axis('off')\n",
    "axs[1][1].set_title('Classification report', y=0.8,pad=-24)\n",
    "# add the table\n",
    "the_table = axs[1][1].table(cellText=df_report.values, colLabels=df_report.columns ,loc='center')\n",
    "the_table.auto_set_font_size(False)\n",
    "the_table.set_fontsize(16)\n",
    "the_table.scale(1.2, 2)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_list = [7,9,11,13,15,20,25,30,35]\n",
    "for i in K_list:\n",
    "  job = f\"NPM1_K{i}\"\n",
    "  # Load the CSV file into a pandas DataFrame\n",
    "  csv_path = f\"/home/weber50432/AML_image_processing/MIL_slide_level/output/{job}/predictions.csv\"\n",
    "  df = pd.read_csv(csv_path, sep=\",\", encoding=\"utf-8\")\n",
    "  # Get the true labels and predicted labels\n",
    "  y_true = df['target']\n",
    "  y_pred = df['prediction']\n",
    "  # Get the gene mutation probabilities\n",
    "  y_score = df['probability']\n",
    "  cm = metrics.confusion_matrix(y_true, y_pred, normalize='true')\n",
    "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])\n",
    "  # Compute the false positive rate, true positive rate and thresholds for the ROC curve\n",
    "  fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "  # Compute the area under the ROC curve\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "  # show the accuracy, precision, recall and f1-score\n",
    "  # print(metrics.classification_report(y_true, y_pred))\n",
    "  # Create a figure with two subplots\n",
    "  fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "  # set the title of the figure\n",
    "  fig.suptitle(f'{job} prediction results', fontsize=16)\n",
    "  # Plot the confusion matrix in the first subplot\n",
    "  cm_display.plot(ax=axs[0][0], cmap=plt.cm.Blues)\n",
    "  axs[0][0].set_title('confusion matrix')\n",
    "  axs[0][0].text(-0.15, 1.1, '(a)', transform=axs[0][0].transAxes, size=14)\n",
    "  # Plot the ROC curve in the second subplot\n",
    "  axs[0][1].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "  axs[0][1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "  axs[0][1].set_xlim([0.0, 1.0])\n",
    "  axs[0][1].set_ylim([0.0, 1.05])\n",
    "  axs[0][1].set_xlabel('False Positive Rate')\n",
    "  axs[0][1].set_ylabel('True Positive Rate')\n",
    "  axs[0][1].set_title('ROC')\n",
    "  axs[0][1].legend(loc=\"lower right\")\n",
    "  axs[0][1].text(-0.15, 1, '(b)', transform=axs[0][1].transAxes, size=14)\n",
    "\n",
    "  # Plot the precision-recall curve in the third subplot\n",
    "  precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
    "  f1 = f1_score(y_true, y_pred)\n",
    "  axs[1][0].plot(recall, precision, color='darkorange', lw=2, label='PR curve (F1 = %0.2f)' % f1)\n",
    "  axs[1][0].set_xlim([0.0, 1.05])\n",
    "  axs[1][0].set_ylim([0.0, 1.05])\n",
    "  axs[1][0].set_xlabel('Recall')\n",
    "  axs[1][0].set_ylabel('Precision')\n",
    "  axs[1][0].text(-0.12, 1.1, '(c)', transform=axs[1][0].transAxes, size=14)\n",
    "  axs[1][0].legend(loc=\"lower right\")\n",
    "  axs[1][0].set_title('Precision-Recall curve')\n",
    "  # show the accuracy, precision, recall and f1-score in a table form at the fourth subplot\n",
    "  report = metrics.classification_report(y_true, y_pred, output_dict=True)\n",
    "  df_report = pd.DataFrame(report).transpose()\n",
    "  df_report.reset_index(drop=False, inplace=True)\n",
    "  df_report.rename(columns={'index':''}, inplace=True)\n",
    "  df_report.replace(to_replace=\"0\", value='False', inplace=True)\n",
    "  df_report.replace(to_replace=\"1\", value='True', inplace=True)\n",
    "  # remove the last two rows(avg)\n",
    "  df_report.drop(df_report.tail(2).index, inplace=True)\n",
    "  df_report = df_report.round(2)\n",
    "  axs[1][1].text(-0.12,0.9, '(d)', transform=axs[1][1].transAxes, size=14)\n",
    "  axs[1][1].axis('tight')\n",
    "  axs[1][1].axis('off')\n",
    "  axs[1][1].set_title('Classification report', y=0.8,pad=-24)\n",
    "  # add the table\n",
    "  the_table = axs[1][1].table(cellText=df_report.values, colLabels=df_report.columns ,loc='center')\n",
    "  the_table.auto_set_font_size(False)\n",
    "  the_table.set_fontsize(16)\n",
    "  the_table.scale(1.2, 2)\n",
    "\n",
    "  # Show the plot\n",
    "  # plt.show()\n",
    "  fig.savefig(f\"/home/weber50432/AML_image_processing/imgs/{job}.png\", dpi=100)\n",
    "  # don't show the plot\n",
    "  plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,40,5):  \n",
    "  job = f\"NPM1_balance_K{i}\"\n",
    "  # Load the CSV file into a pandas DataFrame\n",
    "  csv_path = f\"/home/weber50432/AML_image_processing/MIL_slide_level/output/{job}/predictions.csv\"\n",
    "  df = pd.read_csv(csv_path, sep=\",\", encoding=\"utf-8\")\n",
    "  # Get the true labels and predicted labels\n",
    "  y_true = df['target']\n",
    "  y_pred = df['prediction']\n",
    "  # Get the gene mutation probabilities\n",
    "  y_score = df['probability']\n",
    "  cm = metrics.confusion_matrix(y_true, y_pred, normalize='true')\n",
    "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])\n",
    "  # Compute the false positive rate, true positive rate and thresholds for the ROC curve\n",
    "  fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "  # Compute the area under the ROC curve\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "  # show the accuracy, precision, recall and f1-score\n",
    "  # print(metrics.classification_report(y_true, y_pred))\n",
    "  # Create a figure with two subplots\n",
    "  fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "  # set the title of the figure\n",
    "  fig.suptitle(f'{job} prediction results', fontsize=16)\n",
    "  # Plot the confusion matrix in the first subplot\n",
    "  cm_display.plot(ax=axs[0][0], cmap=plt.cm.Blues)\n",
    "  axs[0][0].set_title('confusion matrix')\n",
    "  axs[0][0].text(-0.15, 1.1, '(a)', transform=axs[0][0].transAxes, size=14)\n",
    "  # Plot the ROC curve in the second subplot\n",
    "  axs[0][1].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "  axs[0][1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "  axs[0][1].set_xlim([0.0, 1.0])\n",
    "  axs[0][1].set_ylim([0.0, 1.05])\n",
    "  axs[0][1].set_xlabel('False Positive Rate')\n",
    "  axs[0][1].set_ylabel('True Positive Rate')\n",
    "  axs[0][1].set_title('ROC')\n",
    "  axs[0][1].legend(loc=\"lower right\")\n",
    "  axs[0][1].text(-0.15, 1, '(b)', transform=axs[0][1].transAxes, size=14)\n",
    "\n",
    "  # Plot the precision-recall curve in the third subplot\n",
    "  precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
    "  f1 = f1_score(y_true, y_pred)\n",
    "  axs[1][0].plot(recall, precision, color='darkorange', lw=2, label='PR curve (F1 = %0.2f)' % f1)\n",
    "  axs[1][0].set_xlim([0.0, 1.05])\n",
    "  axs[1][0].set_ylim([0.0, 1.05])\n",
    "  axs[1][0].set_xlabel('Recall')\n",
    "  axs[1][0].set_ylabel('Precision')\n",
    "  axs[1][0].text(-0.12, 1.1, '(c)', transform=axs[1][0].transAxes, size=14)\n",
    "  axs[1][0].legend(loc=\"lower right\")\n",
    "  axs[1][0].set_title('Precision-Recall curve')\n",
    "  # show the accuracy, precision, recall and f1-score in a table form at the fourth subplot\n",
    "  report = metrics.classification_report(y_true, y_pred, output_dict=True)\n",
    "  df_report = pd.DataFrame(report).transpose()\n",
    "  df_report.reset_index(drop=False, inplace=True)\n",
    "  df_report.rename(columns={'index':''}, inplace=True)\n",
    "  df_report.replace(to_replace=\"0\", value='False', inplace=True)\n",
    "  df_report.replace(to_replace=\"1\", value='True', inplace=True)\n",
    "  # remove the last two rows(avg)\n",
    "  df_report.drop(df_report.tail(2).index, inplace=True)\n",
    "  df_report = df_report.round(2)\n",
    "  axs[1][1].text(-0.12,0.9, '(d)', transform=axs[1][1].transAxes, size=14)\n",
    "  axs[1][1].axis('tight')\n",
    "  axs[1][1].axis('off')\n",
    "  axs[1][1].set_title('Classification report', y=0.8,pad=-24)\n",
    "  # add the table\n",
    "  the_table = axs[1][1].table(cellText=df_report.values, colLabels=df_report.columns ,loc='center')\n",
    "  the_table.auto_set_font_size(False)\n",
    "  the_table.set_fontsize(16)\n",
    "  the_table.scale(1.2, 2)\n",
    "  # Show the plot\n",
    "  # plt.show()\n",
    "  fig.savefig(f\"/home/weber50432/AML_image_processing/imgs/{job}.png\", dpi=100)\n",
    "  plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,40,5):\n",
    "  job =f\"NPM1_balance_V1_K{i}\"\n",
    "  # Load the CSV file into a pandas DataFrame\n",
    "  csv_path = f\"/home/weber50432/AML_image_processing/MIL_slide_level/output/{job}/predictions_testing_without_augmentation.csv\"\n",
    "  df = pd.read_csv(csv_path, sep=\",\", encoding=\"utf-8\")\n",
    "  # Get the true labels and predicted labels\n",
    "  y_true = df['target']\n",
    "  y_pred = df['prediction']\n",
    "  # Get the gene mutation probabilities\n",
    "  y_score = df['probability']\n",
    "  cm = metrics.confusion_matrix(y_true, y_pred, normalize='true')\n",
    "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])\n",
    "  # Compute the false positive rate, true positive rate and thresholds for the ROC curve\n",
    "  fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "  # Compute the area under the ROC curve\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "  # show the accuracy, precision, recall and f1-score\n",
    "  # print(metrics.classification_report(y_true, y_pred))\n",
    "  # Create a figure with two subplots\n",
    "  fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "  # set the title of the figure\n",
    "  fig.suptitle(f'{job} prediction results(without testing data upsampling)', fontsize=16)\n",
    "  # Plot the confusion matrix in the first subplot\n",
    "  cm_display.plot(ax=axs[0][0], cmap=plt.cm.Blues)\n",
    "  axs[0][0].set_title('confusion matrix')\n",
    "  axs[0][0].text(-0.15, 1.1, '(a)', transform=axs[0][0].transAxes, size=14)\n",
    "  # Plot the ROC curve in the second subplot\n",
    "  axs[0][1].plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "  axs[0][1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "  axs[0][1].set_xlim([0.0, 1.0])\n",
    "  axs[0][1].set_ylim([0.0, 1.05])\n",
    "  axs[0][1].set_xlabel('False Positive Rate')\n",
    "  axs[0][1].set_ylabel('True Positive Rate')\n",
    "  axs[0][1].set_title('ROC')\n",
    "  axs[0][1].legend(loc=\"lower right\")\n",
    "  axs[0][1].text(-0.15, 1, '(b)', transform=axs[0][1].transAxes, size=14)\n",
    "\n",
    "  # Plot the precision-recall curve in the third subplot\n",
    "  precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
    "  f1 = f1_score(y_true, y_pred)\n",
    "  axs[1][0].plot(recall, precision, color='darkorange', lw=2, label='PR curve (F1 = %0.2f)' % f1)\n",
    "  axs[1][0].set_xlim([0.0, 1.05])\n",
    "  axs[1][0].set_ylim([0.0, 1.05])\n",
    "  axs[1][0].set_xlabel('Recall')\n",
    "  axs[1][0].set_ylabel('Precision')\n",
    "  axs[1][0].text(-0.12, 1.1, '(c)', transform=axs[1][0].transAxes, size=14)\n",
    "  axs[1][0].legend(loc=\"lower right\")\n",
    "  axs[1][0].set_title('Precision-Recall curve')\n",
    "  # show the accuracy, precision, recall and f1-score in a table form at the fourth subplot\n",
    "  report = metrics.classification_report(y_true, y_pred, output_dict=True)\n",
    "  df_report = pd.DataFrame(report).transpose()\n",
    "  df_report.reset_index(drop=False, inplace=True)\n",
    "  df_report.rename(columns={'index':''}, inplace=True)\n",
    "  df_report.replace(to_replace=\"0\", value='False', inplace=True)\n",
    "  df_report.replace(to_replace=\"1\", value='True', inplace=True)\n",
    "  # remove the last two rows(avg)\n",
    "  df_report.drop(df_report.tail(2).index, inplace=True)\n",
    "  df_report = df_report.round(2)\n",
    "  axs[1][1].text(-0.12,0.9, '(d)', transform=axs[1][1].transAxes, size=14)\n",
    "  axs[1][1].axis('tight')\n",
    "  axs[1][1].axis('off')\n",
    "  axs[1][1].set_title('Classification report', y=0.8,pad=-24)\n",
    "  # add the table\n",
    "  the_table = axs[1][1].table(cellText=df_report.values, colLabels=df_report.columns ,loc='center')\n",
    "  the_table.auto_set_font_size(False)\n",
    "  the_table.set_fontsize(16)\n",
    "  the_table.scale(1.2, 2)\n",
    "\n",
    "  # Show the plot\n",
    "  # plt.show()\n",
    "  fig.savefig(f\"/home/weber50432/AML_image_processing/imgs/{job}_without_testing_upsampling.png\", dpi=100)\n",
    "  plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
