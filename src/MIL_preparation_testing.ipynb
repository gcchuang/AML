{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import utility_functions as uf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# default setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_size = 512\n",
    "target_gene = \"NPM1\"\n",
    "split_size = 500\n",
    "level = 0\n",
    "# left_proportion = 0.6\n",
    "# shrink_proportion = 0.15\n",
    "slide_path = \"/home/exon_storage1/aml_slide/\"\n",
    "info_file_path = \"/home/weber50432/AML_image_processing/code_use_csv/changeSlideName.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original data generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集樣本數量：343\n",
      "positive target：60\n",
      "驗證集樣本數量：115\n",
      "positive target：20\n",
      "測試集樣本樣量：116\n",
      "positive target：21\n"
     ]
    }
   ],
   "source": [
    "slide_list = uf.get_slides_list_number(f\"{slide_path}ROI_level0_pixel512/\")\n",
    "target = uf.get_targets_list(target_gene,slide_list,info_file_path)\n",
    "X = np.array(slide_list)\n",
    "y = np.array(target)\n",
    "# 將數據集按比例 6:2:2 分為訓練集、驗證集和測試集\n",
    "# 找到標籤為1的樣本索引\n",
    "positive_indices = np.where(y == 1)[0]\n",
    "\n",
    "# 使用 train_test_split 函数分割樣本\n",
    "# 首先將標籤為1的樣本分成訓練集、驗證集和測試集\n",
    "train_pos, val_pos_test = train_test_split(positive_indices, test_size=0.4)\n",
    "val_pos, test_pos = train_test_split(val_pos_test, test_size=0.5)\n",
    "\n",
    "# 接下來將標籤為0的樣本分成訓練集、驗證集和測試集\n",
    "train_neg, val_neg_test, y_train, y_val_test = train_test_split(np.where(y == 0)[0], y[np.where(y == 0)[0]], test_size=0.4)\n",
    "val_neg, test_neg, y_val, y_test = train_test_split(val_neg_test, y_val_test, test_size=0.5)\n",
    "\n",
    "# 將訓練集、驗證集和測試集的索引合併起來\n",
    "train_indices = sorted(np.concatenate((train_pos, train_neg)))\n",
    "val_indices = sorted(np.concatenate((val_pos, val_neg)))\n",
    "test_indices = sorted(np.concatenate((test_pos, test_neg)))\n",
    "\n",
    "# 根據索引提取對應的數據和標籤\n",
    "X_train = X[train_indices].tolist()\n",
    "X_val = X[val_indices].tolist()\n",
    "X_test = X[test_indices].tolist()\n",
    "y_train = y[train_indices].tolist()\n",
    "y_val = y[val_indices].tolist()\n",
    "y_test = y[test_indices].tolist()\n",
    "\n",
    "# 計算各個集合的樣本數量\n",
    "print(f\"訓練集樣本數量：{len(X_train)}\")\n",
    "print(f\"positive target：{y_train.count(1)}\")\n",
    "print(f\"驗證集樣本數量：{len(X_val)}\")\n",
    "print(f\"positive target：{y_val.count(1)}\")\n",
    "print(f\"測試集樣本樣量：{len(X_test)}\")\n",
    "print(f\"positive target：{y_test.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slide A9 is processing...\n",
      "slide A12 is processing...\n",
      "slide A22 is processing...\n",
      "slide A26 is processing...\n",
      "slide A60 is processing...\n",
      "slide A99 is processing...\n",
      "slide A102 is processing...\n",
      "slide A103 is processing...\n",
      "slide A104 is processing...\n",
      "slide A105 is processing...\n",
      "slide A106 is processing...\n",
      "slide A108 is processing...\n",
      "slide A121 is processing...\n",
      "slide A124 is processing...\n",
      "slide A129 is processing...\n",
      "slide A131 is processing...\n",
      "slide A135 is processing...\n",
      "slide A136 is processing...\n",
      "slide A146 is processing...\n",
      "slide A147 is processing...\n",
      "slide A151 is processing...\n",
      "slide A152 is processing...\n",
      "slide A153 is processing...\n",
      "slide A154 is processing...\n",
      "slide A156 is processing...\n",
      "slide A159 is processing...\n",
      "slide A160 is processing...\n",
      "slide A161 is processing...\n",
      "slide A162 is processing...\n",
      "slide A167 is processing...\n",
      "slide A169 is processing...\n",
      "slide A171 is processing...\n",
      "slide A173 is processing...\n",
      "slide A177 is processing...\n",
      "slide A180 is processing...\n",
      "slide A197 is processing...\n",
      "slide A199 is processing...\n",
      "slide A200 is processing...\n",
      "slide A201 is processing...\n",
      "slide A203 is processing...\n",
      "slide A206 is processing...\n",
      "slide A208 is processing...\n",
      "slide A209 is processing...\n",
      "slide A230 is processing...\n",
      "slide A232 is processing...\n",
      "slide A233 is processing...\n",
      "slide A234 is processing...\n",
      "slide A235 is processing...\n",
      "slide A238 is processing...\n",
      "slide A239 is processing...\n",
      "slide A241 is processing...\n",
      "slide A242 is processing...\n",
      "slide A244 is processing...\n",
      "slide A257 is processing...\n",
      "slide A273 is processing...\n",
      "slide A292 is processing...\n",
      "slide A314 is processing...\n",
      "slide A317 is processing...\n",
      "slide A322 is processing...\n",
      "slide A323 is processing...\n",
      "slide A326 is processing...\n",
      "slide A330 is processing...\n",
      "slide A331 is processing...\n",
      "slide A334 is processing...\n",
      "slide A336 is processing...\n",
      "slide A337 is processing...\n",
      "slide A338 is processing...\n",
      "slide A342 is processing...\n",
      "slide A343 is processing...\n",
      "slide A345 is processing...\n",
      "slide A347 is processing...\n",
      "slide A348 is processing...\n",
      "slide A352 is processing...\n",
      "slide A358 is processing...\n",
      "slide A360 is processing...\n",
      "slide A361 is processing...\n",
      "slide A362 is processing...\n",
      "slide A363 is processing...\n",
      "slide A365 is processing...\n",
      "slide A366 is processing...\n",
      "slide A369 is processing...\n",
      "slide A370 is processing...\n",
      "slide A371 is processing...\n",
      "slide A374 is processing...\n",
      "slide A377 is processing...\n",
      "slide A378 is processing...\n",
      "slide A380 is processing...\n",
      "slide A382 is processing...\n",
      "slide A383 is processing...\n",
      "slide A384 is processing...\n",
      "slide A385 is processing...\n",
      "slide A387 is processing...\n",
      "slide A390 is processing...\n",
      "slide A393 is processing...\n",
      "slide A395 is processing...\n",
      "slide A396 is processing...\n",
      "slide A398 is processing...\n",
      "slide A404 is processing...\n",
      "slide A405 is processing...\n",
      "slide A406 is processing...\n",
      "slide A408 is processing...\n",
      "slide A409 is processing...\n",
      "slide A410 is processing...\n",
      "slide A412 is processing...\n",
      "slide A413 is processing...\n",
      "slide A420 is processing...\n",
      "slide A422 is processing...\n",
      "slide A423 is processing...\n",
      "slide A424 is processing...\n",
      "slide A425 is processing...\n",
      "slide A428 is processing...\n",
      "slide A431 is processing...\n",
      "slide A437 is processing...\n",
      "slide A440 is processing...\n",
      "slide A441 is processing...\n",
      "slide A443 is processing...\n",
      "slide A445 is processing...\n",
      "slide A446 is processing...\n",
      "slide A447 is processing...\n",
      "slide A448 is processing...\n",
      "slide A449 is processing...\n",
      "slide A453 is processing...\n",
      "slide A455 is processing...\n",
      "slide A456 is processing...\n",
      "slide A459 is processing...\n",
      "slide A461 is processing...\n",
      "slide A462 is processing...\n",
      "slide A463 is processing...\n",
      "slide A465 is processing...\n",
      "slide A467 is processing...\n",
      "slide A471 is processing...\n",
      "slide A472 is processing...\n",
      "slide A475 is processing...\n",
      "slide A476 is processing...\n",
      "slide A477 is processing...\n",
      "slide A478 is processing...\n",
      "slide A482 is processing...\n",
      "slide A486 is processing...\n",
      "slide A487 is processing...\n",
      "slide A488 is processing...\n",
      "slide A490 is processing...\n",
      "slide A491 is processing...\n",
      "slide A494 is processing...\n",
      "slide A506 is processing...\n",
      "slide A507 is processing...\n",
      "slide A509 is processing...\n",
      "slide A512 is processing...\n",
      "slide A525 is processing...\n",
      "slide A527 is processing...\n",
      "slide A531 is processing...\n",
      "slide A534 is processing...\n",
      "slide A536 is processing...\n",
      "slide A539 is processing...\n",
      "slide A550 is processing...\n",
      "slide A554 is processing...\n",
      "slide A560 is processing...\n",
      "slide A568 is processing...\n",
      "slide A570 is processing...\n",
      "slide A588 is processing...\n",
      "slide A590 is processing...\n",
      "slide A592 is processing...\n",
      "slide A594 is processing...\n",
      "slide A596 is processing...\n",
      "slide A603 is processing...\n",
      "slide A612 is processing...\n",
      "slide A617 is processing...\n",
      "slide A620 is processing...\n",
      "slide A622 is processing...\n",
      "slide A623 is processing...\n",
      "slide A628 is processing...\n",
      "slide A630 is processing...\n",
      "slide A632 is processing...\n",
      "slide A639 is processing...\n",
      "slide A640 is processing...\n",
      "slide A642 is processing...\n",
      "slide A643 is processing...\n",
      "slide A647 is processing...\n",
      "slide A651 is processing...\n",
      "slide A656 is processing...\n",
      "slide A657 is processing...\n",
      "slide A658 is processing...\n",
      "slide A662 is processing...\n",
      "slide A664 is processing...\n",
      "slide A665 is processing...\n",
      "slide A667 is processing...\n",
      "slide A668 is processing...\n",
      "slide A669 is processing...\n",
      "slide A670 is processing...\n",
      "slide A673 is processing...\n",
      "slide A714 is processing...\n",
      "slide A716 is processing...\n",
      "slide A717 is processing...\n",
      "slide A718 is processing...\n",
      "slide A719 is processing...\n",
      "slide A721 is processing...\n",
      "slide A722 is processing...\n",
      "slide A723 is processing...\n",
      "slide A724 is processing...\n",
      "slide A727 is processing...\n",
      "slide A729 is processing...\n",
      "slide A732 is processing...\n",
      "slide A734 is processing...\n",
      "slide A735 is processing...\n",
      "slide A738 is processing...\n",
      "slide A740 is processing...\n",
      "slide A742 is processing...\n",
      "slide A743 is processing...\n",
      "slide A745 is processing...\n",
      "slide A755 is processing...\n",
      "slide A757 is processing...\n",
      "slide A758 is processing...\n",
      "slide A759 is processing...\n",
      "slide A760 is processing...\n",
      "slide A761 is processing...\n",
      "slide A762 is processing...\n",
      "slide A763 is processing...\n",
      "slide A764 is processing...\n",
      "slide A767 is processing...\n",
      "slide A770 is processing...\n",
      "slide A771 is processing...\n",
      "slide A782 is processing...\n",
      "slide A794 is processing...\n",
      "slide A795 is processing...\n",
      "slide A797 is processing...\n",
      "slide A799 is processing...\n",
      "slide A806 is processing...\n",
      "slide A808 is processing...\n",
      "slide A809 is processing...\n",
      "slide A810 is processing...\n",
      "slide A811 is processing...\n",
      "slide A812 is processing...\n",
      "slide A813 is processing...\n",
      "slide A816 is processing...\n",
      "slide A819 is processing...\n",
      "slide A820 is processing...\n",
      "slide A821 is processing...\n",
      "slide A822 is processing...\n",
      "slide A824 is processing...\n",
      "slide A826 is processing...\n",
      "slide A832 is processing...\n",
      "slide A833 is processing...\n",
      "slide A836 is processing...\n",
      "slide A837 is processing...\n",
      "slide A839 is processing...\n",
      "slide A843 is processing...\n",
      "slide A844 is processing...\n",
      "slide A846 is processing...\n",
      "slide A847 is processing...\n",
      "slide A852 is processing...\n",
      "slide A869 is processing...\n",
      "slide A872 is processing...\n",
      "slide A876 is processing...\n",
      "slide A882 is processing...\n",
      "slide A886 is processing...\n",
      "slide A902 is processing...\n",
      "slide A914 is processing...\n",
      "slide A916 is processing...\n",
      "slide A936 is processing...\n",
      "slide A938 is processing...\n",
      "slide A942 is processing...\n",
      "slide A943 is processing...\n",
      "slide A944 is processing...\n",
      "slide A946 is processing...\n",
      "slide A949 is processing...\n",
      "slide A953 is processing...\n",
      "slide A957 is processing...\n",
      "slide A958 is processing...\n",
      "slide A962 is processing...\n",
      "slide A967 is processing...\n",
      "slide A974 is processing...\n",
      "slide A978 is processing...\n",
      "slide A981 is processing...\n",
      "slide A982 is processing...\n",
      "slide A985 is processing...\n",
      "slide A989 is processing...\n",
      "slide A990 is processing...\n",
      "slide A991 is processing...\n",
      "slide A997 is processing...\n",
      "slide A1001 is processing...\n",
      "slide A1002 is processing...\n",
      "slide A1005 is processing...\n",
      "slide A1007 is processing...\n",
      "slide A1013 is processing...\n",
      "slide A1016 is processing...\n",
      "slide A1019 is processing...\n",
      "slide A1024 is processing...\n",
      "slide A1025 is processing...\n",
      "slide A1032 is processing...\n",
      "slide A1033 is processing...\n",
      "slide A1034 is processing...\n",
      "slide A1038 is processing...\n",
      "slide A1043 is processing...\n",
      "slide A1047 is processing...\n",
      "slide A1049 is processing...\n",
      "slide A1056 is processing...\n",
      "slide A1057 is processing...\n",
      "slide A1058 is processing...\n",
      "slide A1060 is processing...\n",
      "slide A1065 is processing...\n",
      "slide A1066 is processing...\n",
      "slide A1069 is processing...\n",
      "slide A1074 is processing...\n",
      "slide A1076 is processing...\n",
      "slide A1082 is processing...\n",
      "slide A1083 is processing...\n",
      "slide A1102 is processing...\n",
      "slide A1108 is processing...\n",
      "slide A1109 is processing...\n",
      "slide A1112 is processing...\n",
      "slide A1113 is processing...\n",
      "slide A1114 is processing...\n",
      "slide A1115 is processing...\n",
      "slide A1119 is processing...\n",
      "slide A1121 is processing...\n",
      "slide A1122 is processing...\n",
      "slide A1124 is processing...\n",
      "slide A1129 is processing...\n",
      "slide A1132 is processing...\n",
      "slide A1133 is processing...\n",
      "slide A1135 is processing...\n",
      "slide A1140 is processing...\n",
      "slide A1142 is processing...\n",
      "slide A1144 is processing...\n",
      "slide A1146 is processing...\n",
      "slide A1149 is processing...\n",
      "slide A1153 is processing...\n",
      "slide A1155 is processing...\n",
      "slide A1159 is processing...\n",
      "slide A1160 is processing...\n",
      "slide A1161 is processing...\n",
      "slide A1165 is processing...\n",
      "slide A1172 is processing...\n",
      "slide A1175 is processing...\n",
      "slide A1197 is processing...\n",
      "slide A1199 is processing...\n",
      "slide A1201 is processing...\n",
      "slide A1215 is processing...\n",
      "slide A1236 is processing...\n",
      "slide A1239 is processing...\n",
      "slide A1259 is processing...\n",
      "slide A1306 is processing...\n",
      "slide A1308 is processing...\n",
      "slide A1327 is processing...\n"
     ]
    }
   ],
   "source": [
    "target_gene_rename = target_gene.split(\" \")[0]+\"_patch\"\n",
    "output_path =\"/home/weber50432/AML_image_processing/lib/{}\".format(target_gene_rename)\n",
    "# check the output path is exist or not\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "# save the data\n",
    "train_output = {\n",
    "      \"slides\": uf.make_paths_list(\"/staging/biology/b08611005/ROI_level0_pixel512/\",X_train),\n",
    "      \"grid\": uf.get_patches_grid(slide_path+\"ROI_level0_pixel512/\",X_train,patch_num),\n",
    "      \"targets\": y_train,\n",
    "      \"mult\": patches_size/224,\n",
    "      \"level\": level,\n",
    "  }\n",
    "torch.save(train_output, \"{}/{}_train_data.pt\".format(output_path,target_gene_rename))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# upsampling training data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count the total patches number of each WSIs, and save it as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive target: 101, negative target: 473\n"
     ]
    }
   ],
   "source": [
    "slide_list = uf.get_slides_list_number(f\"{slide_path}ROI_level0_pixel512_norm/\")\n",
    "target_list = uf.get_targets_list(target_gene,slide_list,info_file_path)\n",
    "#get the patches number of each slide, and save it as a list\n",
    "df = pd.DataFrame({'Slide': [],\"Target\":[] ,'patches_number': []})\n",
    "for index,slide_num in enumerate(slide_list):\n",
    "    patch_sum = len(os.listdir(f\"{slide_path}ROI_level0_pixel512_norm/A{slide_num}\"))\n",
    "    df.loc[index] = [slide_num,target_list[index],patch_sum]\n",
    "#count the positive and negative slide number\n",
    "print(f\"positive target: {df['Target'].sum()}, negative target: {len(df)-df['Target'].sum()}\")\n",
    "df.to_csv(f\"/home/weber50432/AML_image_processing/lib/{target_gene}_norm_slide_patch_num.csv\",index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 0 : only upsample positive slides before spliting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weber50432/miniconda3/envs/ML/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slide A299 only has 305 patches\n",
      "slide A308 only has 483 patches\n",
      "slide A352 only has 386 patches\n",
      "slide A426 only has 380 patches\n",
      "slide A446 only has 444 patches\n",
      "slide A665 only has 214 patches\n",
      "slide A731 only has 223 patches\n",
      "slide A732 only has 267 patches\n",
      "slide A763 only has 382 patches\n",
      "slide A764 only has 304 patches\n",
      "slide A942 only has 442 patches\n",
      "slide A1053 only has 477 patches\n",
      "slide A1058 only has 451 patches\n",
      "slide A1083 only has 149 patches\n",
      "slide A1102 only has 364 patches\n",
      "slide A1108 only has 424 patches\n",
      "slide A1121 only has 299 patches\n",
      "slide A1140 only has 351 patches\n",
      "slide A1142 only has 288 patches\n",
      "slide A1307 only has 320 patches\n",
      "949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the slides of targets, and show the number of patches\n",
    "slide_list = uf.get_slides_list_number(f\"{slide_path}ROI_level0_pixel512/\")\n",
    "target_list = uf.get_targets_list(target_gene,slide_list,info_file_path)\n",
    "#get the patches number of each slide, and save it as a list\n",
    "df = pd.DataFrame({'Slide': [],\"Target\":[] ,'patches': []})\n",
    "for index,slide_num in enumerate(slide_list):\n",
    "  target = target_list[index]\n",
    "  patch_list = os.listdir(f\"{slide_path}ROI_level0_pixel512/A{slide_num}\")\n",
    "  # if target is positive, split the patches number of the slide and add it to slide_list\n",
    "  if target == 1:\n",
    "    # Shuffle the original list randomly\n",
    "    random.shuffle(patch_list)\n",
    "    #split the patches list into  sublists\n",
    "    sublists = [patch_list[i:i+split_size] for i in range(0, len(patch_list)-len(patch_list)%split_size, split_size)]\n",
    "    # store the sublists into the dataframe\n",
    "    for j, sublist in enumerate(sublists):\n",
    "      df.loc[df.shape[0]] = [f\"A{slide_num}_{j+1}\",target,sublist]\n",
    "  else:\n",
    "    if split_size < len(patch_list):\n",
    "      sublist = random.sample(patch_list,split_size)\n",
    "    else:\n",
    "      sublist = patch_list\n",
    "      print(f\"slide A{slide_num} only has {len(patch_list)} patches\")\n",
    "    df.loc[df.shape[0]] = [f\"A{slide_num}\",target,sublist]\n",
    "# show the length of the dataframe\n",
    "print(len(df))\n",
    "#save the dataframe as a csv file\n",
    "df.to_csv(f\"/home/weber50432/AML_image_processing/lib/{target_gene}_slide_target_patch_num.csv\",index=False)\n",
    "type(df[\"patches\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train positive target: 337, negative target: 327\n",
      "val positive target: 42, negative target: 52\n",
      "test positive target: 99, negative target: 92\n",
      "train: 69.97%, val: 9.91%, test: 20.13%\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "target_gene_rename = target_gene.split(\" \")[0]+\"_patch_500_balanced\"\n",
    "output_path =f\"/home/weber50432/AML_image_processing/lib/{target_gene_rename}\"\n",
    "# df = pd.read_csv(f\"{output_path}/{target_gene}_slide_target_patch_num.csv\")\n",
    "# split the dataframe into train, val, test\n",
    "df_train, df_val = train_test_split(df, test_size=0.3, random_state=100)\n",
    "df_val, df_test = train_test_split(df_val, test_size=0.67, random_state=100)\n",
    "# show the number of positive and negative target in each dataframe\n",
    "print(f\"train positive target: {df_train['Target'].sum()}, negative target: {len(df_train)-df_train['Target'].sum()}\")\n",
    "print(f\"val positive target: {df_val['Target'].sum()}, negative target: {len(df_val)-df_val['Target'].sum()}\")\n",
    "print(f\"test positive target: {df_test['Target'].sum()}, negative target: {len(df_test)-df_test['Target'].sum()}\")\n",
    "# show the proportion of total data number in each dataframe in percentage\n",
    "print(f\"train: {len(df_train)/len(df)*100:.2f}%, val: {len(df_val)/len(df)*100:.2f}%, test: {len(df_test)/len(df)*100:.2f}%\")\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "# save the data\n",
    "train_output = {\n",
    "      \"slides\": df_train['Slide'].tolist(),\n",
    "      \"grid\": df_train['patches'].tolist(),\n",
    "      \"targets\": df_train['Target'].tolist(),\n",
    "      \"mult\": patches_size/224,\n",
    "      \"level\": level,\n",
    "  }\n",
    "torch.save(train_output, \"{}/{}_train_data.pt\".format(output_path,target_gene_rename))\n",
    "val_output = {\n",
    "        \"slides\": df_val['Slide'].tolist(),\n",
    "        \"grid\": df_val['patches'].tolist(),\n",
    "        \"targets\": df_val['Target'].tolist(),\n",
    "        \"mult\": patches_size/224,\n",
    "        \"level\": level,\n",
    "    }\n",
    "torch.save(val_output, \"{}/{}_val_data.pt\".format(output_path,target_gene_rename))\n",
    "test_output = {\n",
    "        \"slides\": df_test['Slide'].tolist(),\n",
    "        \"grid\": df_test['patches'].tolist(),\n",
    "        \"targets\": df_test['Target'].tolist(),\n",
    "        \"mult\": patches_size/224,\n",
    "        \"level\": level,\n",
    "    }\n",
    "torch.save(test_output, \"{}/{}_test_data.pt\".format(output_path,target_gene_rename))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 1 : upsample positive and negative slides before spliting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weber50432/miniconda3/envs/ML/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slide A299 only has 305 patches\n",
      "slide A308 only has 483 patches\n",
      "slide A352 only has 386 patches\n",
      "slide A365 only has 213 patches\n",
      "slide A426 only has 380 patches\n",
      "slide A446 only has 444 patches\n",
      "slide A581 only has 145 patches\n",
      "slide A660 only has 495 patches\n",
      "slide A665 only has 214 patches\n",
      "slide A724 only has 459 patches\n",
      "slide A731 only has 223 patches\n",
      "slide A732 only has 267 patches\n",
      "slide A763 only has 382 patches\n",
      "slide A764 only has 304 patches\n",
      "slide A942 only has 442 patches\n",
      "slide A1053 only has 477 patches\n",
      "slide A1058 only has 451 patches\n",
      "slide A1065 only has 494 patches\n",
      "slide A1083 only has 149 patches\n",
      "slide A1102 only has 364 patches\n",
      "slide A1108 only has 424 patches\n",
      "slide A1121 only has 299 patches\n",
      "slide A1140 only has 351 patches\n",
      "slide A1142 only has 288 patches\n",
      "slide A1175 only has 453 patches\n",
      "slide A1307 only has 320 patches\n",
      "positive target: 579, negative target: 3157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the slides of targets, and show the number of patches\n",
    "split_size = 500\n",
    "target_gene = \"NPM1\"\n",
    "slide_path = \"/home/exon_storage1/aml_slide/\"\n",
    "info_file_path = \"/home/weber50432/AML_image_processing/code_use_csv/changeSlideName.csv\"\n",
    "slide_list = uf.get_slides_list_number(f\"{slide_path}ROI_level0_pixel512/\")\n",
    "target_list = uf.get_targets_list(target_gene,slide_list,info_file_path)\n",
    "#get the patches number of each slide, and save it as a list\n",
    "df = pd.DataFrame({'Slide': [],\"Target\":[] ,'patches': []})\n",
    "for index,slide_num in enumerate(slide_list):\n",
    "  target = target_list[index]\n",
    "  patch_list = os.listdir(f\"{slide_path}ROI_level0_pixel512/A{slide_num}\")\n",
    "  # if patch number is larger than the desired size, split the patches number of the slide and add it to slide_list\n",
    "  if split_size < len(patch_list):\n",
    "    # Shuffle the original list randomly\n",
    "    random.shuffle(patch_list)\n",
    "    #split the patches list into  sublists\n",
    "    sublists = [patch_list[i:i+split_size] for i in range(0, len(patch_list)-len(patch_list)%split_size, split_size)]\n",
    "    # store the sublists into the dataframe\n",
    "    for j, sublist in enumerate(sublists):\n",
    "      df.loc[df.shape[0]] = [f\"A{slide_num}_{j+1}\",target,sublist]\n",
    "  else:\n",
    "    sublist = patch_list\n",
    "    print(f\"slide A{slide_num} only has {len(patch_list)} patches\")\n",
    "  df.loc[df.shape[0]] = [f\"A{slide_num}\",target,sublist]\n",
    "# show positive and negative target number\n",
    "print(f\"positive target: {df['Target'].sum()}, negative target: {len(df)-df['Target'].sum()}\")\n",
    "#save the dataframe as a csv file\n",
    "df.to_csv(f\"/home/weber50432/AML_image_processing/lib/{target_gene}_slide_target_patch_num.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train positive target: 406, negative target: 2209\n",
      "val positive target: 57, negative target: 312\n",
      "test positive target: 116, negative target: 636\n",
      "train: 69.99%, val: 9.88%, test: 20.13%\n"
     ]
    }
   ],
   "source": [
    "target_gene = \"NPM1\"\n",
    "patches_size = 512\n",
    "level = 0\n",
    "target_gene_rename = target_gene.split(\" \")[0]+\"_patch_500_upsampled\"\n",
    "output_path = f\"/home/weber50432/AML_image_processing/lib/{target_gene_rename}\"\n",
    "df = pd.read_csv(f\"{output_path}/{target_gene}_slide_target_patch_num.csv\")\n",
    "df['patches'] = df['patches'].apply(ast.literal_eval)\n",
    "df_train, df_val = train_test_split(df, test_size=0.3, random_state=100)\n",
    "df_val, df_test = train_test_split(df_val, test_size=0.67, random_state=100)\n",
    "# show the number of positive and negative target in each dataframe\n",
    "print(\n",
    "    f\"train positive target: {df_train['Target'].sum()}, negative target: {len(df_train)-df_train['Target'].sum()}\")\n",
    "print(\n",
    "    f\"val positive target: {df_val['Target'].sum()}, negative target: {len(df_val)-df_val['Target'].sum()}\")\n",
    "print(\n",
    "    f\"test positive target: {df_test['Target'].sum()}, negative target: {len(df_test)-df_test['Target'].sum()}\")\n",
    "# show the proportion of total data number in each dataframe in percentage\n",
    "print(f\"train: {len(df_train)/len(df)*100:.2f}%, val: {len(df_val)/len(df)*100:.2f}%, test: {len(df_test)/len(df)*100:.2f}%\")\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "# save the data\n",
    "train_output = {\n",
    "    \"slides\": df_train['Slide'].tolist(),\n",
    "    \"grid\": df_train['patches'].tolist(),\n",
    "    \"targets\": df_train['Target'].tolist(),\n",
    "    \"mult\": patches_size/224,\n",
    "    \"level\": level,\n",
    "}\n",
    "torch.save(\n",
    "    train_output, \"{}/{}_train_data.pt\".format(output_path, target_gene_rename))\n",
    "val_output = {\n",
    "    \"slides\": df_val['Slide'].tolist(),\n",
    "    \"grid\": df_val['patches'].tolist(),\n",
    "    \"targets\": df_val['Target'].tolist(),\n",
    "    \"mult\": patches_size/224,\n",
    "    \"level\": level,\n",
    "}\n",
    "torch.save(val_output, \"{}/{}_val_data.pt\".format(output_path, target_gene_rename))\n",
    "test_output = {\n",
    "    \"slides\": df_test['Slide'].tolist(),\n",
    "    \"grid\": df_test['patches'].tolist(),\n",
    "    \"targets\": df_test['Target'].tolist(),\n",
    "    \"mult\": patches_size/224,\n",
    "    \"level\": level,\n",
    "}\n",
    "torch.save(\n",
    "    test_output, \"{}/{}_test_data.pt\".format(output_path, target_gene_rename))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 2 : split the data first, and then only upsample positive slides in training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train positive target: 69, negative target: 331\n",
      "val positive target: 11, negative target: 45\n",
      "test positive target: 21, negative target: 95\n",
      "train: 69.93%, val: 9.79%, test: 20.28%\n"
     ]
    }
   ],
   "source": [
    "target_gene_rename = target_gene.split(\" \")[0]+\"_patch_500_training_data_augmentation\"\n",
    "output_path =f\"/home/weber50432/AML_image_processing/lib/{target_gene_rename}\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "df = pd.read_csv(f\"/home/weber50432/AML_image_processing/lib/{target_gene}_slide_patch_num.csv\")\n",
    "df_train, df_val = train_test_split(df, test_size=0.3, random_state=1000)\n",
    "df_val, df_test = train_test_split(df_val, test_size=0.67, random_state=1000)\n",
    "# show the number of positive and negative target in each dataframe\n",
    "print(f\"train positive target: {df_train['Target'].sum()}, negative target: {len(df_train)-df_train['Target'].sum()}\")\n",
    "print(f\"val positive target: {df_val['Target'].sum()}, negative target: {len(df_val)-df_val['Target'].sum()}\")\n",
    "print(f\"test positive target: {df_test['Target'].sum()}, negative target: {len(df_test)-df_test['Target'].sum()}\")\n",
    "# show the proportion of total data number in each dataframe in percentage\n",
    "print(f\"train: {len(df_train)/len(df)*100:.2f}%, val: {len(df_val)/len(df)*100:.2f}%, test: {len(df_test)/len(df)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weber50432/miniconda3/envs/ML/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: positive target: 296, negative target: 331\n"
     ]
    }
   ],
   "source": [
    "# training data upsampling\n",
    "# df_train.drop_duplicates(subset=['Slide'], keep='first', inplace=True)\n",
    "df_temp = pd.DataFrame({'Slide': [], \"Target\": [], 'patches': []})\n",
    "for index, row in df_train.iterrows():\n",
    "    slide_num = row['Slide']\n",
    "    target = row['Target']\n",
    "    patch_list = os.listdir(f\"{slide_path}ROI_level0_pixel512/A{slide_num}\")\n",
    "    # if target is positive, split the patches number of the slide and add it to slide_list\n",
    "    if target == 1:\n",
    "        if split_size < len(patch_list):\n",
    "            # Shuffle the original list randomly\n",
    "            random.shuffle(patch_list)\n",
    "            # split the patches list into  sublists\n",
    "            sublists = [patch_list[i:i+split_size]\n",
    "                        for i in range(0, len(patch_list)-len(patch_list) % split_size, split_size)]\n",
    "            # store the sublists into the dataframe\n",
    "            for j, sublist in enumerate(sublists):\n",
    "                df_temp.loc[df_temp.shape[0]] = [\n",
    "                    f\"A{slide_num}_{j+1}\", target, sublist]\n",
    "        else:\n",
    "            df_temp.loc[df_temp.shape[0]] = [\n",
    "                f\"A{slide_num}\", target, patch_list]\n",
    "    else:\n",
    "        if split_size < len(patch_list):\n",
    "            sublist = random.sample(patch_list, split_size)\n",
    "        else:\n",
    "            sublist = patch_list\n",
    "        df_temp.loc[df_temp.shape[0]] = [f\"A{slide_num}\", target, sublist]\n",
    "df_train = df_temp\n",
    "print(\n",
    "    f\"training: positive target: {df_train['Target'].sum()}, negative target: {len(df_train)-df_train['Target'].sum()}\")\n",
    "# save the data as a .pt file\n",
    "train_output = {\n",
    "    \"slides\": df_train['Slide'].tolist(),\n",
    "    \"grid\": df_train['patches'].tolist(),\n",
    "    \"targets\": df_train['Target'].tolist(),\n",
    "    \"mult\": patches_size/224,\n",
    "    \"level\": level,\n",
    "}\n",
    "torch.save(train_output, \"{}/{}_train_data.pt\".format(output_path, target_gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weber50432/miniconda3/envs/ML/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: positive target: 11, negative target: 45\n"
     ]
    }
   ],
   "source": [
    "# validation data, test data without upsampling\n",
    "df_temp = pd.DataFrame({'Slide': [],\"Target\":[] ,'patches': []})\n",
    "for index,row in df_val.iterrows():\n",
    "    slide_num = row['Slide']\n",
    "    target = row['Target']\n",
    "    patch_list = os.listdir(f\"{slide_path}ROI_level0_pixel512/A{slide_num}\")\n",
    "    if split_size < len(patch_list):\n",
    "        sublist = random.sample(patch_list,split_size)\n",
    "    else:\n",
    "        sublist = patch_list\n",
    "    df_temp.loc[df_temp.shape[0]] = [f\"A{slide_num}\",target,sublist]\n",
    "df_val = df_temp\n",
    "print(f\"validation: positive target: {df_val['Target'].sum()}, negative target: {len(df_val)-df_val['Target'].sum()}\")\n",
    "val_output = {\n",
    "        \"slides\": df_val['Slide'].tolist(),\n",
    "        \"grid\": df_val['patches'].tolist(),\n",
    "        \"targets\": df_val['Target'].tolist(),\n",
    "        \"mult\": patches_size/224,\n",
    "        \"level\": level,\n",
    "    }\n",
    "torch.save(val_output, \"{}/{}_val_data.pt\".format(output_path,target_gene))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weber50432/miniconda3/envs/ML/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: positive target: 21, negative target: 95\n"
     ]
    }
   ],
   "source": [
    "# test data without upsampling\n",
    "df_temp = pd.DataFrame({'Slide': [],\"Target\":[] ,'patches': []})\n",
    "for index,row in df_test.iterrows():\n",
    "    slide_num = row['Slide']\n",
    "    target = row['Target']\n",
    "    patch_list = os.listdir(f\"{slide_path}ROI_level0_pixel512/A{slide_num}\")\n",
    "    if split_size < len(patch_list):\n",
    "        sublist = random.sample(patch_list,split_size)\n",
    "    else:\n",
    "        sublist = patch_list\n",
    "    df_temp.loc[df_temp.shape[0]] = [f\"A{slide_num}\",target,sublist]\n",
    "df_test = df_temp\n",
    "print(f\"test: positive target: {df_test['Target'].sum()}, negative target: {len(df_test)-df_test['Target'].sum()}\")\n",
    "test_output = {\n",
    "        \"slides\": df_test['Slide'].tolist(),\n",
    "        \"grid\": df_test['patches'].tolist(),\n",
    "        \"targets\": df_test['Target'].tolist(),\n",
    "        \"mult\": patches_size/224,\n",
    "        \"level\": level,\n",
    "    }\n",
    "torch.save(test_output, \"{}/{}_test_data.pt\".format(output_path,target_gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weber50432/miniconda3/envs/ML/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: positive target: 132, negative target: 95\n"
     ]
    }
   ],
   "source": [
    "# test data with upsampling \n",
    "df_temp = pd.DataFrame({'Slide': [],\"Target\":[] ,'patches': []})\n",
    "for index,row in df_test.iterrows():\n",
    "  slide_num = row['Slide']\n",
    "  target = row['Target']\n",
    "  patch_list = os.listdir(f\"{slide_path}ROI_level0_pixel512/A{slide_num}\")\n",
    "  # if target is positive, split the patches number of the slide and add it to slide_list\n",
    "  if target == 1:\n",
    "      if split_size < len(patch_list):\n",
    "      # Shuffle the original list randomly\n",
    "          random.shuffle(patch_list)\n",
    "          #split the patches list into  sublists\n",
    "          sublists = [patch_list[i:i+split_size] for i in range(0, len(patch_list)-len(patch_list)%split_size, split_size)]\n",
    "          # store the sublists into the dataframe\n",
    "          for j, sublist in enumerate(sublists):\n",
    "              df_temp.loc[df_temp.shape[0]] = [f\"A{slide_num}_{j+1}\",target,sublist]\n",
    "      else:\n",
    "          df_temp.loc[df_temp.shape[0]] = [f\"A{slide_num}\",target,patch_list]\n",
    "  else:\n",
    "      if split_size < len(patch_list):\n",
    "          sublist = random.sample(patch_list,split_size)\n",
    "      else:\n",
    "          sublist = patch_list\n",
    "      df_temp.loc[df_temp.shape[0]] = [f\"A{slide_num}\",target,sublist]\n",
    "df_test_augment = df_temp\n",
    "print(f\"test: positive target: {df_test_augment['Target'].sum()}, negative target: {len(df_test_augment)-df_test_augment['Target'].sum()}\")\n",
    "test_output = {\n",
    "        \"slides\": df_test_augment['Slide'].tolist(),\n",
    "        \"grid\": df_test_augment['patches'].tolist(),\n",
    "        \"targets\": df_test_augment['Target'].tolist(),\n",
    "        \"mult\": patches_size/224,\n",
    "        \"level\": level\n",
    "    }\n",
    "torch.save(test_output, \"{}/{}_test_augment_data.pt\".format(output_path,target_gene))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 3 : split the data first, and then upsample both positive and negative slides in train, val dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train positive target: 69, negative target: 331\n",
      "val positive target: 11, negative target: 45\n",
      "test positive target: 21, negative target: 95\n",
      "train: 69.93%, val: 9.79%, test: 20.28%\n"
     ]
    }
   ],
   "source": [
    "target_gene_rename = target_gene.split(\" \")[0]\n",
    "output_path =f\"/home/weber50432/AML_image_processing/lib/{target_gene_rename}_patch_500_upsampled_V3\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "df = pd.read_csv(f\"/home/weber50432/AML_image_processing/lib/{target_gene}_slide_patch_num.csv\")\n",
    "df_train, df_val = train_test_split(df, test_size=0.3, random_state=1000)\n",
    "df_val, df_test = train_test_split(df_val, test_size=0.67, random_state=1000)\n",
    "# show the number of positive and negative target in each dataframe\n",
    "print(f\"train positive target: {df_train['Target'].sum()}, negative target: {len(df_train)-df_train['Target'].sum()}\")\n",
    "print(f\"val positive target: {df_val['Target'].sum()}, negative target: {len(df_val)-df_val['Target'].sum()}\")\n",
    "print(f\"test positive target: {df_test['Target'].sum()}, negative target: {len(df_test)-df_test['Target'].sum()}\")\n",
    "# show the proportion of total data number in each dataframe in percentage\n",
    "print(f\"train: {len(df_train)/len(df)*100:.2f}%, val: {len(df_val)/len(df)*100:.2f}%, test: {len(df_test)/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weber50432/miniconda3/envs/ML/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: positive target: 296, negative target: 1885\n"
     ]
    }
   ],
   "source": [
    "# training data upsampling\n",
    "# df_train.drop_duplicates(subset=['Slide'], keep='first', inplace=True)\n",
    "df_out = pd.DataFrame({'Slide': [], \"Target\": [], 'patches': []})\n",
    "for index, row in df_train.iterrows():\n",
    "    slide_num = row['Slide']\n",
    "    target = row['Target']\n",
    "    patch_list = os.listdir(f\"{slide_path}ROI_level0_pixel512/A{slide_num}\")\n",
    "    # split the patches number of the slide and add it to slide_list\n",
    "    if split_size < len(patch_list):\n",
    "        # Shuffle the original list randomly\n",
    "        random.shuffle(patch_list)\n",
    "        # split the patches list into  sublists\n",
    "        sublists = [patch_list[i:i+split_size]\n",
    "                    for i in range(0, len(patch_list)-len(patch_list) % split_size, split_size)]\n",
    "        # store the sublists into the dataframe\n",
    "        for j, sublist in enumerate(sublists):\n",
    "            df_out.loc[df_out.shape[0]] = [\n",
    "                f\"A{slide_num}_{j+1}\", target, sublist]\n",
    "    else:\n",
    "        df_out.loc[df_out.shape[0]] = [\n",
    "            f\"A{slide_num}\", target, patch_list]\n",
    "print(\n",
    "    f\"training: positive target: {df_out['Target'].sum()}, negative target: {len(df_out)-df_out['Target'].sum()}\")\n",
    "# save the data as a .pt file\n",
    "train_output = {\n",
    "    \"slides\": df_out['Slide'].tolist(),\n",
    "    \"grid\": df_out['patches'].tolist(),\n",
    "    \"targets\": df_out['Target'].tolist(),\n",
    "    \"mult\": patches_size/224,\n",
    "    \"level\": level,\n",
    "}\n",
    "torch.save(train_output, \"{}/{}_train_data.pt\".format(output_path, target_gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weber50432/miniconda3/envs/ML/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: positive target: 56, negative target: 324\n"
     ]
    }
   ],
   "source": [
    "# validation data upsampling\n",
    "df_out = pd.DataFrame({'Slide': [], \"Target\": [], 'patches': []})\n",
    "for index, row in df_val.iterrows():\n",
    "    slide_num = row['Slide']\n",
    "    target = row['Target']\n",
    "    patch_list = os.listdir(f\"{slide_path}ROI_level0_pixel512/A{slide_num}\")\n",
    "    # split the patches number of the slide and add it to slide_list\n",
    "    if split_size < len(patch_list):\n",
    "        # Shuffle the original list randomly\n",
    "        random.shuffle(patch_list)\n",
    "        # split the patches list into  sublists\n",
    "        sublists = [patch_list[i:i+split_size]\n",
    "                    for i in range(0, len(patch_list)-len(patch_list) % split_size, split_size)]\n",
    "        # store the sublists into the dataframe\n",
    "        for j, sublist in enumerate(sublists):\n",
    "            df_out.loc[df_out.shape[0]] = [\n",
    "                f\"A{slide_num}_{j+1}\", target, sublist]\n",
    "    else:\n",
    "        df_out.loc[df_out.shape[0]] = [\n",
    "            f\"A{slide_num}\", target, patch_list]\n",
    "print(f\"validation: positive target: {df_out['Target'].sum()}, negative target: {len(df_out)-df_out['Target'].sum()}\")\n",
    "val_output = {\n",
    "    \"slides\": df_out['Slide'].tolist(),\n",
    "    \"grid\": df_out['patches'].tolist(),\n",
    "    \"targets\": df_out['Target'].tolist(),\n",
    "    \"mult\": patches_size/224,\n",
    "    \"level\": level,\n",
    "}\n",
    "torch.save(val_output, \"{}/{}_val_data.pt\".format(output_path, target_gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weber50432/miniconda3/envs/ML/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: positive target: 21, negative target: 95\n"
     ]
    }
   ],
   "source": [
    "# test data without upsampling\n",
    "df_temp = pd.DataFrame({'Slide': [],\"Target\":[] ,'patches': []})\n",
    "for index,row in df_test.iterrows():\n",
    "    slide_num = row['Slide']\n",
    "    target = row['Target']\n",
    "    patch_list = os.listdir(f\"{slide_path}ROI_level0_pixel512/A{slide_num}\")\n",
    "    if split_size < len(patch_list):\n",
    "        sublist = random.sample(patch_list,split_size)\n",
    "    else:\n",
    "        sublist = patch_list\n",
    "    df_temp.loc[df_temp.shape[0]] = [f\"A{slide_num}\",target,sublist]\n",
    "df_test = df_temp\n",
    "print(f\"test: positive target: {df_test['Target'].sum()}, negative target: {len(df_test)-df_test['Target'].sum()}\")\n",
    "test_output = {\n",
    "        \"slides\": df_test['Slide'].tolist(),\n",
    "        \"grid\": df_test['patches'].tolist(),\n",
    "        \"targets\": df_test['Target'].tolist(),\n",
    "        \"mult\": patches_size/224,\n",
    "        \"level\": level,\n",
    "    }\n",
    "torch.save(test_output, \"{}/{}_test_data.pt\".format(output_path,target_gene))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## version 4 : normailzation before version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train positive target: 62, negative target: 339\n",
      "val positive target: 13, negative target: 44\n",
      "test positive target: 26, negative target: 90\n",
      "train: 69.86%, val: 9.93%, test: 20.21%\n"
     ]
    }
   ],
   "source": [
    "target_gene_rename = target_gene.split(\" \")[0]\n",
    "output_path =f\"/home/weber50432/AML_image_processing/lib/{target_gene_rename}_patch_500_upsampled_V4\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "df = pd.read_csv(f\"/home/weber50432/AML_image_processing/lib/{target_gene}_norm_slide_patch_num.csv\")\n",
    "df_train, df_val = train_test_split(df, test_size=0.3, random_state=1000)\n",
    "df_val, df_test = train_test_split(df_val, test_size=0.67, random_state=1000)\n",
    "# show the number of positive and negative target in each dataframe\n",
    "print(f\"train positive target: {df_train['Target'].sum()}, negative target: {len(df_train)-df_train['Target'].sum()}\")\n",
    "print(f\"val positive target: {df_val['Target'].sum()}, negative target: {len(df_val)-df_val['Target'].sum()}\")\n",
    "print(f\"test positive target: {df_test['Target'].sum()}, negative target: {len(df_test)-df_test['Target'].sum()}\")\n",
    "# show the proportion of total data number in each dataframe in percentage\n",
    "print(f\"train: {len(df_train)/len(df)*100:.2f}%, val: {len(df_val)/len(df)*100:.2f}%, test: {len(df_test)/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weber50432/miniconda3/envs/ML/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: positive target: 264, negative target: 1934\n"
     ]
    }
   ],
   "source": [
    "# training data upsampling\n",
    "# df_train.drop_duplicates(subset=['Slide'], keep='first', inplace=True)\n",
    "df_out = pd.DataFrame({'Slide': [], \"Target\": [], 'patches': []})\n",
    "for index, row in df_train.iterrows():\n",
    "    slide_num = row['Slide']\n",
    "    target = row['Target']\n",
    "    patch_list = os.listdir(f\"{slide_path}ROI_level0_pixel512_norm/A{slide_num}\")\n",
    "    # split the patches number of the slide and add it to slide_list\n",
    "    if split_size < len(patch_list):\n",
    "        # Shuffle the original list randomly\n",
    "        random.shuffle(patch_list)\n",
    "        # split the patches list into  sublists\n",
    "        sublists = [patch_list[i:i+split_size]\n",
    "                    for i in range(0, len(patch_list)-len(patch_list) % split_size, split_size)]\n",
    "        # store the sublists into the dataframe\n",
    "        for j, sublist in enumerate(sublists):\n",
    "            df_out.loc[df_out.shape[0]] = [\n",
    "                f\"A{slide_num}_{j+1}\", target, sublist]\n",
    "    else:\n",
    "        df_out.loc[df_out.shape[0]] = [\n",
    "            f\"A{slide_num}\", target, patch_list]\n",
    "print(\n",
    "    f\"training: positive target: {df_out['Target'].sum()}, negative target: {len(df_out)-df_out['Target'].sum()}\")\n",
    "# save the data as a .pt file\n",
    "train_output = {\n",
    "    \"slides\": df_out['Slide'].tolist(),\n",
    "    \"grid\": df_out['patches'].tolist(),\n",
    "    \"targets\": df_out['Target'].tolist(),\n",
    "    \"mult\": patches_size/224,\n",
    "    \"level\": level,\n",
    "}\n",
    "torch.save(train_output, \"{}/{}_train_data.pt\".format(output_path, target_gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weber50432/miniconda3/envs/ML/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation: positive target: 72, negative target: 273\n"
     ]
    }
   ],
   "source": [
    "# validation data upsampling\n",
    "df_out = pd.DataFrame({'Slide': [], \"Target\": [], 'patches': []})\n",
    "for index, row in df_val.iterrows():\n",
    "    slide_num = row['Slide']\n",
    "    target = row['Target']\n",
    "    patch_list = os.listdir(f\"{slide_path}ROI_level0_pixel512_norm/A{slide_num}\")\n",
    "    # split the patches number of the slide and add it to slide_list\n",
    "    if split_size < len(patch_list):\n",
    "        # Shuffle the original list randomly\n",
    "        random.shuffle(patch_list)\n",
    "        # split the patches list into  sublists\n",
    "        sublists = [patch_list[i:i+split_size]\n",
    "                    for i in range(0, len(patch_list)-len(patch_list) % split_size, split_size)]\n",
    "        # store the sublists into the dataframe\n",
    "        for j, sublist in enumerate(sublists):\n",
    "            df_out.loc[df_out.shape[0]] = [\n",
    "                f\"A{slide_num}_{j+1}\", target, sublist]\n",
    "    else:\n",
    "        df_out.loc[df_out.shape[0]] = [\n",
    "            f\"A{slide_num}\", target, patch_list]\n",
    "print(f\"validation: positive target: {df_out['Target'].sum()}, negative target: {len(df_out)-df_out['Target'].sum()}\")\n",
    "val_output = {\n",
    "    \"slides\": df_out['Slide'].tolist(),\n",
    "    \"grid\": df_out['patches'].tolist(),\n",
    "    \"targets\": df_out['Target'].tolist(),\n",
    "    \"mult\": patches_size/224,\n",
    "    \"level\": level,\n",
    "}\n",
    "torch.save(val_output, \"{}/{}_val_data.pt\".format(output_path, target_gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weber50432/miniconda3/envs/ML/lib/python3.9/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: positive target: 26, negative target: 90\n"
     ]
    }
   ],
   "source": [
    "# test data without upsampling\n",
    "df_temp = pd.DataFrame({'Slide': [],\"Target\":[] ,'patches': []})\n",
    "for index,row in df_test.iterrows():\n",
    "    slide_num = row['Slide']\n",
    "    target = row['Target']\n",
    "    patch_list = os.listdir(f\"{slide_path}ROI_level0_pixel512_norm/A{slide_num}\")\n",
    "    if split_size < len(patch_list):\n",
    "        sublist = random.sample(patch_list,split_size)\n",
    "    else:\n",
    "        sublist = patch_list\n",
    "    df_temp.loc[df_temp.shape[0]] = [f\"A{slide_num}\",target,sublist]\n",
    "df_test = df_temp\n",
    "print(f\"test: positive target: {df_test['Target'].sum()}, negative target: {len(df_test)-df_test['Target'].sum()}\")\n",
    "test_output = {\n",
    "        \"slides\": df_test['Slide'].tolist(),\n",
    "        \"grid\": df_test['patches'].tolist(),\n",
    "        \"targets\": df_test['Target'].tolist(),\n",
    "        \"mult\": patches_size/224,\n",
    "        \"level\": level,\n",
    "    }\n",
    "torch.save(test_output, \"{}/{}_test_data.pt\".format(output_path,target_gene))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# down sampling base on the WSIs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
